import numpy as np
import torch
from PIL import Image
import cv2
import matplotlib.pyplot as plt
import pykinect_azure as pykinect
import open3d as o3d

from collections import defaultdict
import time

# Grounding DINO
import GroundingDINO.groundingdino.datasets.transforms as T
from GroundingDINO.groundingdino.models import build_model
from GroundingDINO.groundingdino.util.slconfig import SLConfig
from GroundingDINO.groundingdino.util.utils import (
    clean_state_dict, 
    get_phrases_from_posmap
)

# Segment Anything
from segment_anything import (
    sam_model_registry,
    SamPredictor
)


class Pointcloud:
    def __init__(self, points=[], label=None):
        self.points: list = points
        self.label: str = label
        self.time_created: float = time.time()

    def __str__(self):
        return str(self.points)
    
    def reshape(self, w, c):
        return self.points.reshape(w, c)
    
def prepare_image(image: np.ndarray):
    transform = T.Compose([
        T.RandomResize([800], max_size=1333),
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])

    image = image[:, :, :3][:, :, [2, 1, 0]]  # Convert to RGB
    image_pil = Image.fromarray(image)
    image_tensor, _ = transform(image_pil, None)  # 3, h, w
    return image_pil, image_tensor

def load_model(model_config_path, model_checkpoint_path, bert_base_uncased_path, device):
    args = SLConfig.fromfile(model_config_path)
    args.device = device
    args.bert_base_uncased_path = bert_base_uncased_path

    model = build_model(args)
    checkpoint = torch.load(model_checkpoint_path, map_location="cpu")
    load_res = model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
    print(load_res)

    model.eval()
    return model

def get_grounding_output(model, image, caption, box_threshold, text_threshold, with_logits=True, device="cpu"):
    caption = caption.lower().strip()
    if not caption.endswith("."):
        caption += "."

    model = model.to(device)
    image = image.to(device)

    with torch.no_grad():
        outputs = model(image[None], captions=[caption])
        logits = outputs["pred_logits"].cpu().sigmoid()[0]
        boxes = outputs["pred_boxes"].cpu()[0]

    filt_mask = logits.max(dim=1)[0] > box_threshold
    logits_filt = logits[filt_mask]
    boxes_filt = boxes[filt_mask]

    tokenized = model.tokenizer(caption)
    pred_phrases = [
        get_phrases_from_posmap(logit > text_threshold, tokenized, model.tokenizer) +
        (f"({str(logit.max().item())[:4]})" if with_logits else "")
        for logit, _ in zip(logits_filt, boxes_filt)
    ]

    return boxes_filt, pred_phrases

def show_mask(mask, ax, random_color=False):
    color = (
        np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
        if random_color else np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
    )
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

def show_box(box, ax, label):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))
    ax.text(x0, y0, label)

if __name__ == "__main__":
    # Configuration paths and thresholds
    GROUNDING_DINO_CONFIG = "GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py"
    GROUNDING_DINO_CHECKPOINT = "groundingdino_swint_ogc.pth"
    SAM_CHECKPOINT = "sam_vit_h_4b8939.pth"
    BOX_THRESHOLD = 0.3
    TEXT_THRESHOLD = 0.25
    DEVICE = "cuda"
    BERT_BASE_UNCASED_PATH = None

    # Kinect configuration
    device_config = pykinect.default_configuration
    device_config.color_format = pykinect.K4A_IMAGE_FORMAT_COLOR_BGRA32
    device_config.color_resolution = pykinect.K4A_COLOR_RESOLUTION_720P
    device_config.depth_mode = pykinect.K4A_DEPTH_MODE_NFOV_2X2BINNED

    pykinect.initialize_libraries()
    kinect = pykinect.start_device(config=device_config)

    ## NFOV
    cx = 638.4906616210938
    cy = 364.21429443359375
    fx = 614.5958251953125
    fy = 614.3775634765625

    ## WFOV
    # cx = 957.9860229492188
    # cy = 546.5714111328125
    # fx = 921.8936767578125
    # fy = 921.5663452148438

    K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]]) ## takes us from 3d point to 2d pixel coords
    K_inv = np.eye(4)
    K_inv[:3, :3] = K
    K_inv = np.linalg.inv(K_inv) ## takes us from 2d pixel coords to 3d point

    vis = o3d.visualization.Visualizer()

    # Load models
    gd_model = load_model(GROUNDING_DINO_CONFIG, GROUNDING_DINO_CHECKPOINT, BERT_BASE_UNCASED_PATH, device=DEVICE)
    sam_model = SamPredictor(sam_model_registry["vit_h"](checkpoint=SAM_CHECKPOINT).to(DEVICE))

    # Stores the masks generated by sam, object --> list[mask]
    scene = {}
    pointclouds = defaultdict(list) # object -> list[Pointcloud]

    while True:
        capture = kinect.update()
        ret_color, color_image = capture.get_color_image()  # (720, 1280, 4)
        ret_depth, depth_image = capture.get_depth_image()
        if not (ret_color and ret_depth):
            continue

        ## Need to resize color image to be same dimensions as depth image
        color_image = cv2.resize(color_image, (depth_image.shape[1], depth_image.shape[0]))

        image_pil, image_tensor = prepare_image(color_image)
        boxes, pred_phrases = get_grounding_output(
            gd_model, image_tensor, "backpack. box. chair. table.", BOX_THRESHOLD, TEXT_THRESHOLD, device=DEVICE
        )

        # Image.fromarray(color_image[:, :, [2, 1, 0]]).save("./images/color_image1.png")
        # Image.fromarray(depth_image).save("./images/depth_image1.png")

        sam_model.set_image(color_image[:, :, :3])

        # Adjust boxes for SAM and visualization
        W, H = image_pil.size
        for i in range(boxes.size(0)):
            boxes[i] *= torch.Tensor([W, H, W, H])
            boxes[i][:2] -= boxes[i][2:] / 2
            boxes[i][2:] += boxes[i][:2]

        transformed_boxes = sam_model.transform.apply_boxes_torch(boxes, color_image.shape[:2]).to(DEVICE)
        masks, _, _ = sam_model.predict_torch(
            point_coords=None, point_labels=None, boxes=transformed_boxes.to(DEVICE), multimask_output=False
        )

        ## Associate outputs of sam and grounding dino
        color_image = np.array(image_pil)
        for mask, box, label in zip(masks, boxes, pred_phrases):
            mask = mask[0].cpu().numpy()
            x0, y0 = box[0], box[1]
            label = label[:label.index('(')]

            if label not in scene:
                scene[label] = []
            scene[label].append(mask)

            ## How to use
            # color_pixels = color_image * mask[:, :, None]
            # depth_pixels = depth_image * mask[:, :]

        pointcloud_mask = np.zeros_like(depth_image)
        for obj in scene:
            for mask in scene[obj]:
                pointcloud_mask |= mask

        ## Generating point clouds
        masked_color = color_image * pointcloud_mask[:, :, None]
        masked_depths = depth_image * pointcloud_mask

        fig, axes = plt.subplots(1, 3, figsize=(10,5))
        axes[0].axis('off'); axes[1].axis('off'); axes[2].axis('off')
        axes[0].imshow(masked_color); axes[1].imshow(masked_depths); axes[2].imshow(color_image)
        plt.show()

        ## Viz
        # for obj in scene:
        #     for mask in scene[obj]:
        #         color_pixels = color_image * mahumanfigsize=(10, 5))
        #         axes[0].axis('off'); axes[1].axis('off')
        #         axes[0].imshow(color_pixels); axes[1].imshow(depth_pixels)
        #         plt.tight_layout()
        #         plt.show()

        rows, cols = depth_image.shape
        # main_pointcloud = np.zeros((rows, cols, 3))
        for obj in scene:
            for mask in scene[obj]:

                pointcloud = Pointcloud(np.zeros((rows, cols, 3)), obj)

                for u in range(0, cols):
                    for v in range(0, rows):
                        depth_value = masked_depths[v, u] * 0.001
                        if depth_value == 0.0:
                            continue

                        uv_h = np.array([u, v, 1., 1 / depth_value])
                        point = depth_value * (K_inv @ uv_h.T)
                        if (np.isnan(point[0]) or np.isnan(point[1]) or np.isnan(point[2])):
                            continue

                        pointcloud.points[v, u] = point[:3]
                        # main_pointcloud[v, u] = point[:3]

                pointclouds[obj].append(pointcloud)
        
        # main_pointcloud = main_pointcloud.reshape((rows * cols, 3))
        pointcloud = pointclouds['chair'][0]
        pointcloud = pointcloud.reshape(rows * cols, 3)
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(pointcloud)
        vis.create_window(window_name="Point cloud", width=800, height=800)
        vis.add_geometry(pcd)
        vis.run()
        vis.destroy_window()
